{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc971f4b-8e5f-430a-ad08-0e14d92f60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bc02d-d673-4a9e-b6bf-6ac775c08d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Set up API credentials\n",
    "API_KEY = 'AIzaSyA5iu941DlosGOEJ5ZJv_wa_AQ5KdCJG2I'\n",
    "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "YOUTUBE_API_VERSION = 'v3'\n",
    "\n",
    "# Initialize the YouTube API client\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
    "\n",
    "def get_video_comments(video_id, max_comments=500):\n",
    "    \"\"\"\n",
    "    Retrieve up to `max_comments` comments from a given YouTube video using the official API.\n",
    "    Comments are sorted by relevance (likes/engagement).\n",
    "    Returns a list of dictionaries, each containing author name, comment text, publish date, and like count.\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    # Progress bar for visualization\n",
    "    pbar = tqdm(total=max_comments, desc=\"Fetching comments\")\n",
    "    \n",
    "    while len(comments) < max_comments:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,\n",
    "            order='relevance',  # Sort comments by relevance (e.g., likes/engagement)\n",
    "            pageToken=next_page_token)\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Parse response and extract desired fields\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'text': comment['textDisplay'],\n",
    "                'published_at': comment['publishedAt'],\n",
    "                'like_count': comment['likeCount']\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            if len(comments) >= max_comments:\n",
    "                break\n",
    "        \n",
    "        # Check and set the next page token for paginated results\n",
    "        if 'nextPageToken' in response:\n",
    "            next_page_token = response['nextPageToken']\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9256b3a-c8fa-42d2-9c0c-31933f070704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for scraping comments from multiple videos\n",
    "def crawl_multiple_videos(video_ids, max_comments_per_video=300):\n",
    "    \"\"\"\n",
    "    Fetch comments from a list of YouTube video IDs.\n",
    "    For each video, retrieve up to `max_comments_per_video` comments using get_video_comments().\n",
    "    Returns a list of dictionaries with augmented video_id field for tracking provenance.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for vid in video_ids:\n",
    "        print(f\"Fetching comments for video {vid} ...\")\n",
    "        comments = get_video_comments(vid, max_comments=max_comments_per_video)\n",
    "        # Add video ID to each comment for traceability\n",
    "        for c in comments:\n",
    "            c['video_id'] = vid\n",
    "        all_comments.extend(comments)\n",
    "    return all_comments\n",
    "\n",
    "# List of selected video IDs (GPT-related)\n",
    "video_ids = [\n",
    "    'jPhJbKBuNnA', 'GiaNp0u_swU', 'xswXcoh0UXs', 'MmFLDvOFLW0', 'kopoLzvh5jY', 'oc6RV5c1yd0', 'boJG84Jcf-4',\n",
    "    '_8yVOC4ciXc', '_x9AwxfjxvE', 'PqbB07n_uQ4', 'MirzFk_DSiI', '_nSmkyDNulk', 'vgYi3Wr7v_g', 'lEcg6AJ6DVY',\n",
    "    '50W4YeQdnSg', 'iBfQTnA2n2s'\n",
    "]\n",
    "\n",
    "# Batch comment retrieval from multiple videos\n",
    "comments = crawl_multiple_videos(video_ids, max_comments_per_video=200)\n",
    "\n",
    "# Save to CSV for downstream analysis\n",
    "pd.DataFrame(comments).to_csv('youtube_multi_videos_comments.csv', index=False)\n",
    "\n",
    "# Load the data for subsequent processing (data cleaning, sentiment analysis, etc.)\n",
    "df = pd.read_csv('youtube_multi_videos_comments.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fa8a4-bb66-42db-bd07-044c0c20c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove HTML tags, special entities, and excessive whitespace.\n",
    "    Converts input to string for consistent processing.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', str(text))\n",
    "    # Replace special HTML entities\n",
    "    text = text.replace('&amp;', '&').replace('&quot;', '\"').replace('&#39;', \"'\")\n",
    "    # Replace newlines and carriage returns with space, strip leading/trailing whitespace\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    return text\n",
    "\n",
    "# Load raw comment data\n",
    "df = pd.read_csv('youtube_multi_videos_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Apply text cleaning function to the 'text' column\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Filter: retain comments with more than 5 characters\n",
    "df = df[df['text'].str.len() > 5]\n",
    "\n",
    "# Filter: remove comments containing only symbols or whitespace\n",
    "df = df[~df['text'].str.match(r'^[<>\\|\\/\\\\\\-_=\\.\\s]+$')]\n",
    "\n",
    "# Remove duplicate comments based on text field\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Filter out advertising phrases\n",
    "df = df[~df['text'].str.contains('buy now|check my channel', case=False)]\n",
    "\n",
    "# Retain only English-language comments using langdetect\n",
    "from langdetect import detect\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df_en = df[df['text'].apply(is_english)]\n",
    "\n",
    "# Format published date column as datetime; extract 'year_month' feature for aggregation\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')\n",
    "df['year_month'] = df['published_at'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a434cc4-dc06-4509-80bc-b37b65f09e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "comment_counts = df.groupby('year_month').size().reset_index(name='count')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(comment_counts['year_month'], comment_counts['count'], color='steelblue')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('YouTube Comments Count by Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006e612-f7bf-42b5-a147-f2bde8e807fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc707a-0768-41d5-892c-9320602159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "# Simulate user visits for anti-scraping purposes; note: cookies have a limited validity period and must be updated regularly.\n",
    "headers = {\n",
    "    \"cookies\": \"header_theme_version=CLOSE; enable_web_push=DISABLE; ...\",  # (Use valid, updated cookies here)\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0\",\n",
    "}\n",
    "\n",
    "# Comment endpoint for Bilibili API\n",
    "url = \"https://api.bilibili.com/x/v2/reply/wbi/main\"\n",
    "\n",
    "# For each video, you need to update 'oid', 'w_rid', and 'wts' by inspecting the network traffic in the developer console.\n",
    "# How-to: Open Developer Tools > Network > Filter for 'main?oid', scroll the webpage to load multiple pages of comments.\n",
    "# Each page contains 20 comments. The 'pagination_str' for the first page is '{\"offset\":\"\"}', \n",
    "# for subsequent pages use '{\"offset\":\"xxxxxxxx\"}', and paste it into the function parameters, wrapped in single quotes.\n",
    "# Each page requires unique values for 'w_rid' and 'wts'.\n",
    "\n",
    "\"\"\"\n",
    "Example usage:\n",
    "w_rid = ['e29839f3c8ce192cbe0fc7a103c8811f', 'a0f76273c229dbb4fbd94c3b5ca17947', '8ee30604c03902d1c174bbd26ce72b0d']\n",
    "wts = ['1763656706', '1763656711', '1763656713']\n",
    "all_comments = get_bili_comments(442080192, '{\"offset\":\"CAESEDE4MDU5ODQ0NjY5Njk5NDQiAggB\"}', wts, w_rid)\n",
    "\"\"\"\n",
    "\n",
    "def get_bili_comments(oid, offset, wts, w_rid):\n",
    "    \"\"\"\n",
    "    Scrape Bilibili comments for a specific video (oid).\n",
    "    Handles pagination and anti-crawling tokens (w_rid, wts).\n",
    "    Returns a list of comments with author, publish date, text, like count, and video ID.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    params = {\n",
    "        'oid': oid,\n",
    "        'type': '1',\n",
    "        'mode': '3',\n",
    "        'pagination_str': '{\"offset\":\"\"}',\n",
    "        'plat': '1',\n",
    "        'seek_rpid': '',\n",
    "        'web_location': '1315875',\n",
    "        'w_rid': w_rid[0],\n",
    "        'wts': wts[0]\n",
    "    }\n",
    "    response = requests.get(url=url, params=params, headers=headers)\n",
    "    json_data = response.json()\n",
    "    if json_data['code'] == 0:\n",
    "        print('Page 1 complete:', str(json_data)[:20], '...')\n",
    "    else:\n",
    "        print('Failed to fetch page 1')\n",
    "    replies = json_data['data']['replies']\n",
    "    n = 0\n",
    "    for index in replies:\n",
    "        n += 1\n",
    "        ctime = index['ctime']\n",
    "        date = str(datetime.fromtimestamp(ctime))\n",
    "        dit = {\n",
    "            'author': index['member']['uname'],\n",
    "            'published_at': date,\n",
    "            'text': index['content']['message'],\n",
    "            'like_count': index['like'],\n",
    "            'video_id': oid\n",
    "        }\n",
    "        all_comments.append(dit)\n",
    "    \n",
    "    # Loop through each additional page using updated anti-crawling tokens\n",
    "    for i in range(1, len(wts)): \n",
    "        params = {\n",
    "            'oid': oid,\n",
    "            'type': '1',\n",
    "            'mode': '3',\n",
    "            'pagination_str': offset,\n",
    "            'plat': '1',\n",
    "            'web_location': '1315875',\n",
    "            'w_rid': w_rid[i],\n",
    "            'wts': wts[i]\n",
    "        }\n",
    "        response = requests.get(url=url, params=params, headers=headers)\n",
    "        json_data = response.json()\n",
    "        if json_data['code'] == 0:\n",
    "            print(f\"Page {i+1} complete:\", str(json_data)[:20], '...')\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {i+1}\")\n",
    "            continue\n",
    "        replies = json_data['data']['replies']\n",
    "        n = 0\n",
    "        for index in replies:\n",
    "            n += 1\n",
    "            ctime = index['ctime']\n",
    "            date = str(datetime.fromtimestamp(ctime))\n",
    "            dit = {\n",
    "                'author': index['member']['uname'],\n",
    "                'published_at': date,\n",
    "                'text': index['content']['message'],\n",
    "                'like_count': index['like'],\n",
    "                'video_id': oid\n",
    "            }\n",
    "            all_comments.append(dit)\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f89a7-de1d-4b8f-9a99-b35a29ba0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web crawler with json\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Locate all JSON files containing Bilibili comment data\n",
    "files = glob.glob(os.path.join('bilijson', 'bilijson*.txt'))\n",
    "\n",
    "all_comments = []\n",
    "\n",
    "# Aggregate all comments from the parsed JSON files\n",
    "for fname in tqdm(files, desc=\"Processing JSON files\"):\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    for index in data:\n",
    "        ctime = index.get('ctime')\n",
    "        date = str(datetime.fromtimestamp(ctime)) if ctime else \"\"\n",
    "        dit = {\n",
    "            'author': index['member']['uname'] if 'member' in index else \"\",\n",
    "            'published_at': date,\n",
    "            'text': index.get('content', {}).get('message', \"\"),\n",
    "            'like_count': index.get('like', 0),\n",
    "        }\n",
    "        all_comments.append(dit)\n",
    "\n",
    "# Save all parsed comments to a single CSV file for subsequent analysis\n",
    "df_all = pd.DataFrame(all_comments)\n",
    "df_all.to_csv('bili_videos_comments.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfffc0-9ccd-4a20-b5ac-7e2f6ba84020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "import re\n",
    "\n",
    "# Load raw Bilibili comment data\n",
    "df = pd.read_csv('bili_videos_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Clean comment text content\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Filter: retain comments longer than 6 characters\n",
    "df = df[df['text'].str.len() > 6]\n",
    "\n",
    "# Filter: remove comments containing only symbols or whitespace\n",
    "df = df[~df['text'].str.match(r'^[<>\\|\\/\\\\\\-_=\\.\\s]+$')]\n",
    "\n",
    "# Remove duplicate comments based on the text field\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Filter out promotional phrases (e.g., common Bilibili \"triple support\" memes)\n",
    "df = df[~df['text'].str.contains('已三连', case=False)]\n",
    "\n",
    "# Convert publication date string to standardized pandas datetime format\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')\n",
    "\n",
    "# Extract year-month as a new feature for time-based aggregation\n",
    "df['year_month'] = df['published_at'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Save the cleaned dataset for further analysis\n",
    "df.to_csv('bili_videos_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae297b9-8fac-4347-ab59-964698e0b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "comment_counts = df.groupby('year_month').size().reset_index(name='count')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(comment_counts['year_month'], comment_counts['count'], color='steelblue')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Bilibili Comments Count by Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c3199-69c6-4d97-89b2-583c138c582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accc2b6-50e8-45d9-82d2-60a14047a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weibo crawler (sourced from GitHub)\n",
    "# Install dependencies in Anaconda Prompt, then run the main application.\n",
    "# Do NOT execute this cell directly.\n",
    "\n",
    "# Clone the repository\n",
    "git clone https://github.com/zhouyi207/WeiBoCrawler.git\n",
    "\n",
    "# Install required Python packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Launch the Streamlit web crawler interface\n",
    "streamlit run web/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29f2f7-5cab-43e8-a282-fff23dce0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "import re\n",
    "\n",
    "# Load raw Bilibili comment data\n",
    "df = pd.read_csv('weibo_searchs.csv', encoding='utf-8')\n",
    "\n",
    "# Clean comment text content\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Filter: retain comments longer than 6 characters\n",
    "df = df[df['text'].str.len() > 6]\n",
    "\n",
    "# Filter: remove comments containing only symbols or whitespace\n",
    "df = df[~df['text'].str.match(r'^[<>\\|\\/\\\\\\-_=\\.\\s]+$')]\n",
    "\n",
    "# Remove duplicate comments based on the text field\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Filter out promotional phrases (e.g., common Bilibili \"triple support\" memes)\n",
    "df = df[~df['text'].str.contains('已三连', case=False)]\n",
    "\n",
    "# Convert publication date string to standardized pandas datetime format\n",
    "df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')\n",
    "\n",
    "# Extract year-month as a new feature for time-based aggregation\n",
    "df['year_month'] = df['published_at'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Save the cleaned dataset for further analysis\n",
    "df.to_csv('weibo_searchs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaba7eb-8b60-4545-b86b-deb27b3e41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "comment_counts = df.groupby('year_month').size().reset_index(name='count')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(comment_counts['year_month'], comment_counts['count'], color='steelblue')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Bilibili Comments Count by Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecdf13-3cbd-4318-9cc4-a256c9f0b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weight by weight = log(like_counts+1)\n",
    "import numpy as np\n",
    "# weibo\n",
    "df = pd.read_csv('weibo_searchs.csv', encoding='utf-8')\n",
    "df['weight'] = np.log1p(df['like_count'])\n",
    "df.to_csv('weibo_searchs.csv', index=False)\n",
    "\n",
    "#bili\n",
    "df = pd.read_csv('bili_videos_comments.csv', encoding='utf-8')\n",
    "df['weight'] = np.log1p(df['like_count'])\n",
    "df.to_csv('bili_videos_comments.csv', index=False)\n",
    "\n",
    "#youtube\n",
    "df = pd.read_csv('youtube_multi_videos_comments.csv', encoding='utf-8')\n",
    "df['weight'] = np.log1p(df['like_count'])\n",
    "df.to_csv('youtube_multi_videos_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
